{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwgOy+4EBQAEH4AFOzW6oa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chema74/AI-Portfolio-2025/blob/main/08_Feature_Selection_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìà M√≥dulo 8: Selecci√≥n de Caracter√≠sticas y Pipelines\n",
        "\n",
        "La **Selecci√≥n de Caracter√≠sticas (Feature Selection)** y la **Ingenier√≠a de Caracter√≠sticas (Feature Engineering)** son procesos que buscan optimizar el conjunto de datos de entrada ($X$) para mejorar la precisi√≥n y reducir el tiempo de entrenamiento del modelo.\n",
        "\n",
        "## Conceptos Clave\n",
        "\n",
        "### 1. La Maldici√≥n de la Dimensionalidad (Curse of Dimensionality)\n",
        "* **Definici√≥n:** A medida que el n√∫mero de caracter√≠sticas (dimensiones) aumenta, la cantidad de datos requerida para mantener la misma densidad de informaci√≥n crece exponencialmente.\n",
        "* **Problema:** Un n√∫mero excesivo de caracter√≠sticas irrelevantes o redundantes puede hacer que los algoritmos de ML funcionen peor, sean m√°s lentos y requieran m√°s recursos.\n",
        "\n",
        "### 2. M√©todos de Selecci√≥n de Caracter√≠sticas\n",
        "\n",
        "La selecci√≥n consiste en eliminar las caracter√≠sticas menos informativas:\n",
        "* **M√©todos de Filtro (Filter Methods):** Usan estad√≠sticas (como correlaci√≥n o puntuaci√≥n chi-cuadrado) para evaluar la relaci√≥n de cada caracter√≠stica con la variable objetivo, *independientemente del modelo*. Son r√°pidos.\n",
        "* **M√©todos de Envoltura (Wrapper Methods):** Usan el rendimiento del modelo (ej. precisi√≥n) para evaluar subconjuntos de caracter√≠sticas. Son m√°s lentos, pero a menudo m√°s precisos (ej. Eliminaci√≥n Recursiva de Caracter√≠sticas - RFE).\n",
        "\n",
        "### 3. El Pipeline de Scikit-learn (Flujo de Trabajo)\n",
        "Un **Pipeline** es una herramienta de `sklearn` que encadena pasos de preprocesamiento (como el escalado de datos) y un estimador (el modelo de ML) en un solo objeto.\n",
        "* **Ventaja:** Garantiza que los pasos de preprocesamiento (ej. `StandardScaler` o `fit`) se apliquen consistentemente a los datos de entrenamiento y de prueba, **evitando la fuga de datos (data leakage)**."
      ],
      "metadata": {
        "id": "I1j2dUX-w3HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejercicio, aplicaremos la Eliminaci√≥n Recursiva de Caracter√≠sticas (RFE), un m√©todo de envoltura, y lo implementaremos dentro de un Pipeline para mantener un flujo de trabajo limpio y profesional."
      ],
      "metadata": {
        "id": "o30DTX-Bw9pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# PASO 1: Importar librer√≠as\n",
        "# =======================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE # Importamos la Eliminaci√≥n Recursiva de Caracter√≠sticas\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline # Importamos la herramienta Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"‚úÖ Librer√≠as de Selecci√≥n de Caracter√≠sticas y Pipeline importadas.\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# =======================================================\n",
        "# PASO 2: Preparaci√≥n de Datos con M√∫ltiples Caracter√≠sticas\n",
        "# =======================================================\n",
        "\n",
        "# Usaremos un DataFrame con m√°s variables predictoras (X)\n",
        "datos = {\n",
        "    'Precio_Unitario': [15.5, 15.5, 15.5, 22.0, 10.0, 15.5],\n",
        "    'Dias_Envio': [5, 2, 5, 7, 1, 2],\n",
        "    'Antiguedad_Cliente': [10, 2, 1, 5, 8, 3], # Nueva Caracter√≠stica 1\n",
        "    'Descuento': [0, 1, 0, 1, 0, 0], # Nueva Caracter√≠stica 2 (binaria)\n",
        "    'Cantidad': [3, 1, 3, 2, 4, 1]\n",
        "}\n",
        "df = pd.DataFrame(datos)\n",
        "\n",
        "# Creamos la variable objetivo binaria (Y) como en el M√≥dulo 5 (Clasificaci√≥n)\n",
        "media_cantidad = df['Cantidad'].mean()\n",
        "df['Cantidad_Alta'] = np.where(df['Cantidad'] > media_cantidad, 1, 0)\n",
        "\n",
        "# X: Seleccionamos TODAS las caracter√≠sticas para la selecci√≥n\n",
        "X = df[['Precio_Unitario', 'Dias_Envio', 'Antiguedad_Cliente', 'Descuento']]\n",
        "Y = df['Cantidad_Alta']\n",
        "\n",
        "print(f\"1. Total de caracter√≠sticas iniciales (dimensionalidad): {X.shape[1]}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# PASO 3: Divisi√≥n de Datos\n",
        "# =======================================================\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.3, random_state=42 # Cambiamos el test_size para tener m√°s datos de prueba\n",
        ")\n",
        "\n",
        "print(f\"2. Muestras de entrenamiento: {len(X_train)}\")\n",
        "print(f\"3. Muestras de prueba: {len(X_test)}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# PASO 4: Implementaci√≥n del Pipeline (Escalado + Selecci√≥n RFE + Modelo)\n",
        "# =======================================================\n",
        "\n",
        "# 4a. Definimos el Estimador (Modelo Base) para RFE\n",
        "# Usaremos Regresi√≥n Log√≠stica como base para evaluar las caracter√≠sticas\n",
        "estimator = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# 4b. Definimos el paso de Selecci√≥n de Caracter√≠sticas (RFE)\n",
        "# RFE usar√° el Estimador (RegLog) y elegir√° las 2 mejores caracter√≠sticas de las 4.\n",
        "selector_rfe = RFE(estimator, n_features_to_select=2, step=1)\n",
        "\n",
        "# 4c. Creamos el Pipeline: Encadenamos Escalado, RFE y el Modelo final\n",
        "modelo_pipeline = Pipeline(steps=[\n",
        "    ('escalador', StandardScaler()), # Paso 1: Escalar las variables (vital para LogReg/SVM)\n",
        "    ('seleccion', selector_rfe),     # Paso 2: Seleccionar las 2 mejores variables\n",
        "    ('clasificador', estimator)      # Paso 3: Entrenar el modelo final\n",
        "])\n",
        "\n",
        "# 4d. Entrenar el Pipeline (Aplica todos los pasos en orden, solo a los datos de train)\n",
        "modelo_pipeline.fit(X_train, Y_train)\n",
        "\n",
        "print(\"4. Pipeline entrenado (Escalado, RFE y Log√≠stica aplicados).\")\n",
        "\n",
        "# 4e. Identificar las caracter√≠sticas seleccionadas por RFE\n",
        "# Debemos acceder al paso 'seleccion' del pipeline y luego al selector:\n",
        "columnas_seleccionadas = X.columns[modelo_pipeline.named_steps['seleccion'].support_]\n",
        "\n",
        "print(f\"5. Caracter√≠sticas seleccionadas por RFE: {list(columnas_seleccionadas)}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# PASO 5: Predicci√≥n y Evaluaci√≥n\n",
        "# =======================================================\n",
        "\n",
        "# El pipeline predice aplicando autom√°ticamente el escalado y la selecci√≥n a X_test\n",
        "Y_pred = modelo_pipeline.predict(X_test)\n",
        "precision = accuracy_score(Y_test, Y_pred)\n",
        "\n",
        "print(\"6. Evaluaci√≥n del rendimiento en datos de PRUEBA:\")\n",
        "print(f\"   Precisi√≥n (Accuracy) con Pipeline y RFE: {precision:.4f}\")\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE58JYb4xBNT",
        "outputId": "026011e3-fa6b-4496-a731-5bf51d5cd579"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Librer√≠as de Selecci√≥n de Caracter√≠sticas y Pipeline importadas.\n",
            "----------------------------------------------------------------------\n",
            "1. Total de caracter√≠sticas iniciales (dimensionalidad): 4\n",
            "----------------------------------------------------------------------\n",
            "2. Muestras de entrenamiento: 4\n",
            "3. Muestras de prueba: 2\n",
            "----------------------------------------------------------------------\n",
            "4. Pipeline entrenado (Escalado, RFE y Log√≠stica aplicados).\n",
            "5. Caracter√≠sticas seleccionadas por RFE: ['Precio_Unitario', 'Descuento']\n",
            "----------------------------------------------------------------------\n",
            "6. Evaluaci√≥n del rendimiento en datos de PRUEBA:\n",
            "   Precisi√≥n (Accuracy) con Pipeline y RFE: 1.0000\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}